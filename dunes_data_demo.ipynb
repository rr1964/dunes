{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo of DUNES data\n",
    "A demonstration of how to use the DUNES_data Python module to stream DUNES data for neural networks in PyTorch. \n",
    "This assumes that a user is using the data on INL's HPC systems. (Or that the directory structure matches HPC's: /projects/dunes/[code, large_data])\n",
    "\n",
    "* The LargeIterDataset defines the dataset, much like a PyTorch Dataset. It is not necessary to even import this class. Everything can be defined by just the BatchIterator.\n",
    "* The BatchIterator is a simple way to properly sample and iterate on a LargeIterDataset. It handles all the randomized shuffling of the data.\n",
    "* The function `make_data_dicts()` takes an SNR and a preamble size value and then creates two dictionaries (train, test) of LargeIterDataset objects. These dictionaries (one for train data, one for test data) contains a LargeIterDataset for **each** protocol. \n",
    "* The default `train_ratio` is 0.8.\n",
    "* It is not hard to modify the `make_data_dicts()` function to create a train, validation, and test dictionary. You would then be able to define three iterators. Or you can split the test iterator if you really wanted. \n",
    "* Each iteration of the BatchIterator will return one **batch** of samples over three domains (Time, Frequency, Discrete Cosine Transform), plus the associated label vector of the sample.\n",
    "* You can also use the MacroIterator class directly to get a single sample of data. This would give a single sample in a quadruple (time, freq, DCT, label). It works essentially just like a BatchIterator with a batch size of one. \n",
    "* **Note:** The premade neural networks all assume a batch dimension, so if you just use the MacroIterator, you will have to use something like `<data_tensor>.unsqueeze(dim=0)` to get the dimensionality correct.\n",
    "* For demonstration purposes, we can also import a simple set of autoencoders (written in PyTorch). Importing these autoencoders is not necessary generally.\n",
    "\n",
    "The data labels are as follows (`make_data_dicts()` also prints this out as a reminder):\n",
    "* Bluetooth: 0\n",
    "* Bluetooth LE: 1\n",
    "* WLAN: 2\n",
    "* WLAN HE: 3\n",
    "* Zigbee: 4\n",
    "\n",
    "At some point, we can add functionality for holding Zigbee out in order to create an \"open\" class as opposed to the closed classifications. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DUNES_data import BatchIterator, make_data_dicts, SimpleConvAE, SimpleLinearAE\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the signal-to-noise-ratio (SNR: 5,10,15,20,25) and then the packet preable size (usually 128 or 256). For simple printout demonstration purposes, you may want to have a preamble size of 4 or 8. For actual model training, you will want it to be much larger. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "snr=25\n",
    "preamb_sz = 128\n",
    "train_ratio = 0.7\n",
    "total_samples = 10000\n",
    "batch_sz=64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the `make_data_dicts()` function not print out the labels, set `explain=False` in the function. Note that these data dictionaries are only created once for the training and test sets. This is important because it means that we can create as many iterators on the train and test data as we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data labels are as follows:\n",
      "\n",
      "\t0: Bluetooth\n",
      "\t1: Bluetooth LE\n",
      "\t2: WLAN\n",
      "\t3: WLAN HE\n",
      "\t4: Zigbee\n"
     ]
    }
   ],
   "source": [
    "train_data_dict, test_data_dict = make_data_dicts(snr=snr, chunk_sz=preamb_sz, train_ratio = train_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "## Using the BatchIterator\n",
    "We can use a BatchIterator to iterate through the data set.\n",
    "\n",
    "#### Only run this block with small batch size and small preamble size.\n",
    "Otherwise the print out will be a mile long. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batchIter = BatchIterator(train_data_dict, batch_sz=batch_sz, num_samp=int(total_samples*train_ratio))\n",
    "test_batchIter = BatchIterator(test_data_dict, batch_sz=batch_sz, num_samp=int(total_samples*(1-train_ratio)))\n",
    "\n",
    "for t,f,d,l in train_batchIter:\n",
    "    print(\"Time domain:\\n\", t)\n",
    "    print(\"Freq domain:\\n\", f)\n",
    "    print(\"DCT domain:\\n\", d)\n",
    "    print(\"Label:\", l)\n",
    "    print(\"\\n-----------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BatchIterator and Model\n",
    "We can use the BatchIterator to feed data into a simple PyTorch model. Throughout this notebook, the quadruple `t,f,d,l` refers to time, frequency, DCT, label. This is the quadruple of data. \n",
    "\n",
    "There is nothing too magical about the learning rates or layer sizes. Anecdotally, the feature extractor for the frequency domain seems to need a smaller leraning rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are using cuda as our device.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SimpleConvAE(\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv1d(2, 4, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (2): Conv1d(4, 8, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    (3): LeakyReLU(negative_slope=0.05, inplace=True)\n",
       "  )\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (decoder): Sequential(\n",
       "    (0): ConvTranspose1d(8, 4, kernel_size=(3,), stride=(2,), padding=(1,), output_padding=(1,))\n",
       "    (1): LeakyReLU(negative_slope=0.05, inplace=True)\n",
       "    (2): ConvTranspose1d(4, 2, kernel_size=(3,), stride=(2,), padding=(1,), output_padding=(1,))\n",
       "    (3): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_ae = SimpleConvAE()\n",
    "t_optimizer = Adam(t_ae.parameters(), lr=0.01)\n",
    "f_ae = SimpleConvAE()\n",
    "f_optimizer = Adam(f_ae.parameters(), lr=0.001)\n",
    "d_ae = SimpleConvAE()\n",
    "d_optimizer = Adam(d_ae.parameters(), lr=0.01)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"We are using {device} as our device.\")\n",
    "t_ae.to(device)\n",
    "f_ae.to(device)\n",
    "d_ae.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in theory you can train all three autoencoders (t,f,d) in a single loop. You just have to be careful to keep everything well annotated. We could also implement early stopping if we wanted. May not be necessary at this point.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [2/20],\n",
      "Avg. time domain loss: 0.050349\n",
      "Avg. frequency domain loss: 10.004286\n",
      "Avg. DCT domain loss: 0.058740\n",
      "-----------------------------------------------\n",
      "\n",
      "\n",
      "Epoch [4/20],\n",
      "Avg. time domain loss: 0.046727\n",
      "Avg. frequency domain loss: 9.868808\n",
      "Avg. DCT domain loss: 0.056868\n",
      "-----------------------------------------------\n",
      "\n",
      "\n",
      "Epoch [6/20],\n",
      "Avg. time domain loss: 0.045294\n",
      "Avg. frequency domain loss: 9.853449\n",
      "Avg. DCT domain loss: 0.055346\n",
      "-----------------------------------------------\n",
      "\n",
      "\n",
      "Epoch [8/20],\n",
      "Avg. time domain loss: 0.043657\n",
      "Avg. frequency domain loss: 9.846097\n",
      "Avg. DCT domain loss: 0.054403\n",
      "-----------------------------------------------\n",
      "\n",
      "\n",
      "Epoch [10/20],\n",
      "Avg. time domain loss: 0.043363\n",
      "Avg. frequency domain loss: 9.840994\n",
      "Avg. DCT domain loss: 0.053265\n",
      "-----------------------------------------------\n",
      "\n",
      "\n",
      "Epoch [12/20],\n",
      "Avg. time domain loss: 0.043018\n",
      "Avg. frequency domain loss: 9.838065\n",
      "Avg. DCT domain loss: 0.052748\n",
      "-----------------------------------------------\n",
      "\n",
      "\n",
      "Epoch [14/20],\n",
      "Avg. time domain loss: 0.042680\n",
      "Avg. frequency domain loss: 9.836264\n",
      "Avg. DCT domain loss: 0.052563\n",
      "-----------------------------------------------\n",
      "\n",
      "\n",
      "Epoch [16/20],\n",
      "Avg. time domain loss: 0.042579\n",
      "Avg. frequency domain loss: 9.834827\n",
      "Avg. DCT domain loss: 0.052403\n",
      "-----------------------------------------------\n",
      "\n",
      "\n",
      "Epoch [18/20],\n",
      "Avg. time domain loss: 0.042527\n",
      "Avg. frequency domain loss: 9.832360\n",
      "Avg. DCT domain loss: 0.052315\n",
      "-----------------------------------------------\n",
      "\n",
      "\n",
      "Epoch [20/20],\n",
      "Avg. time domain loss: 0.042508\n",
      "Avg. frequency domain loss: 9.830447\n",
      "Avg. DCT domain loss: 0.052255\n",
      "-----------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Number of training epochs\n",
    "num_epochs = 20\n",
    "\n",
    "## Training loop for PyTorch\n",
    "for epoch in range(num_epochs):\n",
    "    total_t_loss = 0.0\n",
    "    total_f_loss = 0.0\n",
    "    total_d_loss = 0.0\n",
    "    num_batches = 0\n",
    "    train_batchIter = BatchIterator(train_data_dict, batch_sz=batch_sz, num_samp=int(total_samples*train_ratio))\n",
    "    \n",
    "    # Iterate over batches in the training dataset.\n",
    "    for  t,f,d,l in train_batchIter:\n",
    "        ## Zero the gradients for all three models\n",
    "        t_optimizer.zero_grad()\n",
    "        f_optimizer.zero_grad()\n",
    "        d_optimizer.zero_grad()\n",
    "        \n",
    "        t,f,d,l = t.to(device), f.to(device), d.to(device), l.to(device)\n",
    "        \n",
    "        # Forward pass all models\n",
    "        t_out = t_ae(t.float())\n",
    "        f_out = f_ae(f.float())\n",
    "        d_out = d_ae(d.float())\n",
    "        \n",
    "        # Compute the loss of each model separately. \n",
    "        t_loss = criterion(t_out, t.float())\n",
    "        f_loss = criterion(f_out, f.float())\n",
    "        d_loss = criterion(d_out, d.float())\n",
    "        \n",
    "        # Backward pass\n",
    "        t_loss.backward()\n",
    "        f_loss.backward()\n",
    "        d_loss.backward()\n",
    "        \n",
    "        # Update the parameters\n",
    "        t_optimizer.step()\n",
    "        f_optimizer.step()\n",
    "        d_optimizer.step()\n",
    "        \n",
    "        # Accumulate the total loss\n",
    "        total_t_loss += t_loss.item()\n",
    "        total_f_loss += f_loss.item()\n",
    "        total_d_loss += d_loss.item()\n",
    "        num_batches += 1\n",
    "    \n",
    "    # Compute the average loss for the epoch\n",
    "    ## print(f\"This is the number of batches so far: {num_batches}\")\n",
    "    avg_t_loss = total_t_loss / num_batches\n",
    "    avg_f_loss = total_f_loss / num_batches\n",
    "    avg_d_loss = total_d_loss / num_batches\n",
    "    \n",
    "    # Print the average loss for the epoch\n",
    "    if (epoch+1) % 2 ==0:\n",
    "        print(f\"\\nEpoch [{epoch+1}/{num_epochs}],\\nAvg. time domain loss: {avg_t_loss:.6f}\")\n",
    "        print(f\"Avg. frequency domain loss: {avg_f_loss:.6f}\\nAvg. DCT domain loss: {avg_d_loss:.6f}\")\n",
    "        print(\"-----------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------\n",
    "## Feature Extraction\n",
    "Once the autoencoder (feature extractor) is trained, then we can use that trained feature extractor to give us the features. \n",
    "* Use the `<SimpleConvAE>.features(domain_data)` function to get these features. \n",
    "* If the model was trained on GPU (cuda) then the data will also need to be sent to that device. \n",
    "* Use `detach()` if you need to detatch the data from the device. \n",
    "* Because PyTorch is picky about data types, we need to cast the data to float using `.float()`\n",
    "* Keep in mind that whatever your batch size will also be the first dimension of the output of the `features()` function.\n",
    "* Thus if we have a batch size of 16, we will get *16* feature vectors for each input batch (also of size 16 of course)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batchIter = BatchIterator(test_data_dict, batch_sz=batch_sz, num_samp=int(total_samples*(1-train_ratio)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t,f,d,l = next(test_batchIter)\n",
    "t,f,d,l = t.to(device), f.to(device), d.to(device), l.to(device)\n",
    "##print(f\"The label is {l}\")\n",
    "t_ae.features(t.float()).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fully connected autoencoder. Does not seem to be as effective as the CNN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------------\n",
      "It is assumed that input data has separated real and imaginary parts.\n",
      "\n",
      "Data will be flattened to an input size of 256 unless a value for factor arg has been set.\n",
      "-----------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SimpleLinearAE(\n",
       "  (encoder): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Linear(in_features=64, out_features=16, bias=True)\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=16, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=128, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=128, out_features=256, bias=True)\n",
       "    (5): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lae = SimpleLinearAE(preamb_sz)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = Adam(lae.parameters(), lr=0.005)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "lae.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Avg. Loss: 0.0876\n",
      "Epoch [2/20], Avg. Loss: 0.0818\n",
      "Epoch [3/20], Avg. Loss: 0.0818\n",
      "Epoch [4/20], Avg. Loss: 0.0818\n",
      "Epoch [5/20], Avg. Loss: 0.0817\n",
      "Epoch [6/20], Avg. Loss: 0.0817\n",
      "Epoch [7/20], Avg. Loss: 0.0817\n",
      "Epoch [8/20], Avg. Loss: 0.0817\n",
      "Epoch [9/20], Avg. Loss: 0.0817\n",
      "Epoch [10/20], Avg. Loss: 0.0819\n",
      "Epoch [11/20], Avg. Loss: 0.0818\n",
      "Epoch [12/20], Avg. Loss: 0.0817\n",
      "Epoch [13/20], Avg. Loss: 0.0817\n",
      "Epoch [14/20], Avg. Loss: 0.0818\n",
      "Epoch [15/20], Avg. Loss: 0.0818\n",
      "Epoch [16/20], Avg. Loss: 0.0818\n",
      "Epoch [17/20], Avg. Loss: 0.0818\n",
      "Epoch [18/20], Avg. Loss: 0.0817\n",
      "Epoch [19/20], Avg. Loss: 0.0817\n",
      "Epoch [20/20], Avg. Loss: 0.0817\n"
     ]
    }
   ],
   "source": [
    "## Number of training epochs\n",
    "num_epochs = 20\n",
    "\n",
    "## Training loop for PyTorch\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    train_batchIter = BatchIterator(train_data_dict, batch_sz=batch_sz, num_samp=int(total_samples*train_ratio))\n",
    "    \n",
    "    # Iterate over batches in the training dataset\n",
    "    for t,f,d,l in train_batchIter:\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        t,f,d,l = t.to(device), f.to(device), d.to(device), l.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        output_data = lae(d.float())\n",
    "        #print(d)\n",
    "        #print(output_data)\n",
    "        # Compute the loss\n",
    "        loss = criterion(output_data, d.float())\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate the total loss\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    \n",
    "    # Compute the average loss for the epoch\n",
    "    avg_loss = total_loss / num_batches\n",
    "    \n",
    "    # Print the average loss for the epoch\n",
    "    if (epoch+1) % 1 ==0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Avg. Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Fastai PyTorch CUDA 11.2",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
